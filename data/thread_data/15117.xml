<?xml version='1.0' encoding='utf-8'?>
<doc><id>15111</id><url>http://www.chinanews.com/gj/2023/12-19/10131328.shtml</url><title>美媒：OpenAI发布新安全指南 赋予董事会否决权</title><datatime>2023-12-19 16:44:00</datatime><body>	中新网12月19日电据彭博社报道，当地时间18日，美国人工智能公司OpenAI在其网站上公布一项安全指南，加强其内部安全流程，并赋予董事会对高风险人工智能的否决权。
	据报道，当天，OpenAI发表一系列安全指南，阐释公司计划如何应对最强大人工智能(AI)系统可能引起的极端风险。
	在该指南下，OpenAI只会在确定最新技术的安全性之后才会开始运用。公司将成立一个顾问团队以审核安全报告，再转交公司高层和董事会。虽然公司领导负责做决策，董事会可推翻决定。
	OpenAI在2023年10月宣布成立“备灾小组”，并会持续评估它的AI系统在网络安全、化学威胁、核威胁和生物威胁等四大范畴的表现，同时减低科技可能带来的任何危害。
	具体来说，公司检查的是定义为：能造成上亿美元经济损失，或导致许多个人受伤害或死亡的“灾难性”风险。
	“AI本身没有好坏，我们正在塑造它。”领导“备灾小组”的亚历山大·马德里表示，他的团队会每月提交报告给内部新成立的安全顾问团队，再由阿尔特曼与公司董事会判断分析团队所提交的建议。
	马德里希望，其他公司能利用OpenAI的指引，来评估旗下AI模型的潜在风险。
	2023年3月，包括马斯克、苹果联合创始人斯蒂夫·沃兹尼亚克在内的1000多名人工智能专家和行业高管签署了一份公开信，他们在公开信中呼吁暂停高级人工智能开发，直至此类设计的共享安全协议经由独立专家开发、实施和审核。
</body><key_sentence>据报道，当天，OpenAI发表一系列安全指南，阐释公司计划如何应对最强大人工智能(AI)系统可能引起的极端风险。”领导“备灾小组”的亚历山大·马德里表示，他的团队会每月提交报告给内部新成立的安全顾问团队，再由阿尔特曼与公司董事会判断分析团队所提交的建议。中新网12月19日电据彭博社报道，当地时间18日，美国人工智能公司OpenAI在其网站上公布一项安全指南，加强其内部安全流程，并赋予董事会对高风险人工智能的否决权。OpenAI在2023年10月宣布成立“备灾小组”，并会持续评估它的AI系统在网络安全、化学威胁、核威胁和生物威胁等四大范畴的表现，同时减低科技可能带来的任何危害。</key_sentence></doc>